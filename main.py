from __future__ import annotations
import io
import os
import uuid
import json
import re
from enum import Enum
from typing import Any, Dict, Optional, Union, List
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from fastapi import FastAPI, File, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import OpenAI
import seaborn as sns
from functools import lru_cache
import umap
from sklearn.metrics import roc_curve, auc
from sklearn.decomposition import PCA
from docx import Document
from docx.shared import Inches
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.pagesizes import A4
from datetime import datetime

# ---------------------------
# Load API Key
# ---------------------------
load_dotenv()
api_key = os.getenv("OPENROUTER_API_KEY")
if not api_key:
    raise ValueError("OPENROUTER_API_KEY not found in .env file")

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key,
)

# ---------------------------
# FastAPI Setup
# ---------------------------
app = FastAPI(title="LLM Chart & Dataset QA Service", version="2.5.4")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True,
                   allow_methods=["*"], allow_headers=["*"])
DATASETS: Dict[str, pd.DataFrame] = {}
CHART_HISTORY: Dict[str, List[Dict[str, Any]]] = {}  # {dataset_id: [{"chart_type": str, "image_path": str, "instruction": str}, ...]}

# ---------------------------
# Debug Endpoint
# ---------------------------
@app.get("/debug")
async def debug():
    return {"message": "Server is running", "endpoints": [route.path for route in app.routes]}

# ---------------------------
# Models
# ---------------------------
class ChartType(str, Enum):
    line = "line"
    bar = "bar"
    scatter = "scatter"
    hist = "hist"
    box = "box"
    heatmap = "heatmap"
    violin = "violin"
    swarm = "swarm"
    pie = "pie"
    volcano = "volcano"
    tga = "tga"
    dtg = "dtg"
    cluster_heatmap = "cluster_heatmap"
    exp_curve = "exp_curve"
    log_curve = "log_curve"
    smooth_curve = "smooth_curve"
    logistic_regression = "logistic_regression"
    kaplan_meier = "kaplan_meier"
    umap = "umap"
    roc = "roc"
    pca = "pca"

class OutputFormat(str, Enum):
    png = "png"
    svg = "svg"

class ChartSpec(BaseModel):
    chart_type: ChartType
    x: Optional[str] = None
    y: Optional[Union[str, List[str]]] = None
    group: Optional[str] = None
    bins: Optional[int] = None
    title: Optional[str] = None
    x_label: Optional[str] = None
    y_label: Optional[str] = None
    color: Optional[str] = None
    size: Optional[str] = None
    fdr_col: Optional[str] = None

class RenderRequest(BaseModel):
    dataset_id: str
    instruction: str
    format: OutputFormat = OutputFormat.png

class AskRequest(BaseModel):
    dataset_id: str
    question: str

class ReportRequest(BaseModel):
    dataset_id: str
    customer_name: str
    filename: str
    format: str = "pdf"
    logo_path: Optional[str] = None
    summary: Optional[str] = "Generated by Innov® Labs"
    conclusion: Optional[str] = "Analysis completed successfully."

# ---------------------------
# Helpers
# ---------------------------
@lru_cache(maxsize=1000)
def normalize_column(col: Optional[str], df_columns: tuple) -> Optional[str]:
    """Match user-given column names against normalized df.columns with caching."""
    if not col:
        return None
    target = col.strip().lower()
    for c in df_columns:
        if c == target:
            return c
        def clean(s: str) -> str:
            return re.sub(r"[^a-z0-9]", "", s)
        if clean(c) == clean(target):
            return c
    return None

def pick_default_numeric(df: pd.DataFrame, n=1, exclude=None):
    """Select default numeric columns, excluding specified columns if provided."""
    nums = df.select_dtypes(include=[np.number]).columns.tolist()
    if exclude is not None:
        exclude = [col.strip().lower() for col in (exclude if isinstance(exclude, list) else [exclude])]
        nums = [col for col in nums if col not in exclude]
    return nums[:n] if len(nums) >= n else []

def find_column_by_keywords(df: pd.DataFrame, keywords: list) -> Optional[str]:
    """Find a column containing any of the given keywords (case-insensitive)."""
    for col in df.columns:
        col_lower = col.lower()
        if any(keyword.lower() in col_lower for keyword in keywords):
            return col
    return None

# ---------------------------
# LLM Router for Plotting
# ---------------------------
def llm_router(instruction: str, df: pd.DataFrame) -> Dict[str, Any]:
    system_prompt = (
        "You are an expert assistant that converts plotting instructions into a JSON object containing a ChartSpec and executable Python code for rendering the chart. "
        "Return ONLY valid JSON with two fields: 'spec' (matching the ChartSpec model) and 'code' (a string of syntactically correct Python code to render the chart). "
        "Ensure all code is syntactically valid, with correct parentheses, brackets, and indentation. Double-check for missing or mismatched parentheses/brackets. "
        "Supported chart types: [line, bar, scatter, hist, box, violin, swarm, pie, heatmap, volcano, tga, dtg, cluster_heatmap, exp_curve, log_curve, smooth_curve, logistic_regression, kaplan_meier, umap, roc, pca, radar, funnel]. "
        "If the requested chart type is unclear or unsupported, map it to the closest reasonable type (e.g., 'radar' to a polar plot, 'funnel' to a bar or area plot) or raise an HTTPException with a clear message. "
        "Only use column names that exist in the dataset: " + ", ".join(df.columns.astype(str)) + ". "
        "The code must: "
        "- NOT include any import statements (use pre-provided modules: plt, sns, np, pd, re, curve_fit, UnivariateSpline, gradient, HTTPException, umap, roc_curve, auc, PCA). "
        "- Normalize df columns to lowercase stripped using: df.columns = [c.strip().lower() for c in df.columns]. "
        "- Use normalize_column and pick_default_numeric helpers if needed. "
        "- Use find_column_by_keywords to identify columns by keywords (e.g., ['temp', 'celsius', 'temperature'] for temperature, ['mass', 'weight'] for mass). "
        "- Handle fallback defaults for x, y, etc., using pick_default_numeric with a warning in the error message if used. "
        "- Validate data types and handle missing values: check for numeric columns, ensure no NaN values, and raise HTTPException with specific guidance (e.g., 'Ensure column is numeric and contains no NaN values'). "
        "- Create fig, ax = plt.subplots(figsize=(6, 4)) for most charts, except cluster_heatmap. "
        "- Implement the full chart logic, including any special handling like curve fitting, polar coordinates for radar, or stacked bars for funnel. "
        "- Set titles, labels, legends as per spec, ensuring axis labels are set for heatmaps (e.g., ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')). "
        "- Save to buf: fig.tight_layout(); fig.savefig(buf, format=fmt.value, dpi=300); plt.close(fig). "
        "- For cluster_heatmap, use sns.clustermap, set row_cluster=True, col_cluster=True, and save the figure correctly. "
        "- For logistic_regression, ensure y values are cast to np.float64, check for binary labels (0 or 1), and use np.min and np.max for range calculations. Raise HTTPException if y is not binary with message: 'Ensure y column contains only 0 or 1 values.' "
        "- For smooth_curve, sort x and y by x to ensure x is strictly increasing, remove duplicates, check for at least 3 unique x values, and use UnivariateSpline with a reasonable smoothing factor (e.g., s=len(unique_x)/2). Raise HTTPException if insufficient unique values with message: 'Ensure x column has at least 3 unique values.' "
        "- For umap, use umap.UMAP to reduce numeric columns to 2D, create a scatter plot, and color by 'group' if specified. Drop non-numeric columns and handle missing values with dropna(). "
        "- For roc, use roc_curve and auc from sklearn.metrics, expect 'y' as binary labels and 'x' as predicted probabilities, check for binary y (0 or 1) and numeric x, and plot the ROC curve with AUC in the legend. Raise HTTPException if y is not binary with message: 'Ensure y column contains only 0 or 1 values.' "
        "- For pca, use PCA from sklearn.decomposition to reduce numeric columns to 2D, create a scatter plot, and color by 'group' if specified. Drop non-numeric columns and handle missing values with dropna(). "
        "- For radar charts, use polar coordinates with plt.subplot(projection='polar') and ensure data is normalized if needed. "
        "- For funnel charts, use a bar or area plot with decreasing values to represent stages. "
        "- For kaplan_meier, expect 'x' as a time column (numeric) and 'y' as a binary event column (0 or 1, where 1 indicates an event and 0 indicates censoring). Check for numeric x and binary y, calculate survival probabilities using the Kaplan-Meier estimator, and plot a step plot with ax.step. Raise HTTPException if y is not binary with message: 'Ensure y column contains only 0 or 1 values.' "
        "- For tga, expect 'x' as a temperature column and 'y' as a mass column. Use find_column_by_keywords with keywords ['temp', 'celsius', 'temperature'] for x and ['mass', 'weight'] for y. If not found, use normalize_column to match 'temperature', 'temp', 'temp (°c)', 'mass', 'mass (%)'. If still not found, use pick_default_numeric and include a warning in the error message if used. Validate that x and y are numeric and have no NaN values. Plot a line chart of mass vs. temperature. "
        "- For dtg, expect 'x' as a temperature column and 'y' as a mass or derivative column. Use find_column_by_keywords with keywords ['temp', 'celsius', 'temperature'] for x and ['mass', 'weight', 'dmass', 'derivative'] for y. If not found, use normalize_column to match 'temperature', 'temp', 'temp (°c)', 'mass', 'mass (%)', 'dmass/dt (%)'. If still not found, use pick_default_numeric and include a warning. Validate that x and y are numeric and have no NaN values. If 'dmass/dt (%)' or a derivative column is found, use it directly; otherwise, calculate the derivative using np.gradient. Plot as a line chart. "
        "- For tga_dtg_combined, expect 'x' as a temperature column and 'y' as a mass column. Use find_column_by_keywords with keywords ['temp', 'celsius', 'temperature'] for x and ['mass', 'weight'] for y, and ['dmass', 'derivative'] for derivative. If not found, use normalize_column to match 'temperature', 'temp', 'temp (°c)', 'mass', 'mass (%)', 'dmass/dt (%)'. If still not found, use pick_default_numeric and include a warning. Validate that x and y are numeric and have no NaN values. Create a figure with two y-axes: one for TGA (mass vs. temperature) and one for DTG (derivative of mass vs. temperature, or 'dmass/dt (%)' if available). Use ax.twinx() for the second axis. "
        "- Raise HTTPException for errors with specific messages (e.g., 'Ensure column <col> is numeric and contains no NaN values', 'Insufficient unique values in <col>, ensure at least 3 unique values', or 'Using default numeric columns; specify columns with keywords like temp or mass in instruction'). "
        "Example output for tga: "
        "{\"spec\": {\"chart_type\": \"tga\", \"x\": \"temp (°c)\", \"y\": \"mass (%)\", \"group\": null, \"bins\": null, \"title\": \"TGA Curve\", \"x_label\": \"Temperature (°C)\", \"y_label\": \"Mass (%)\", \"color\": null, \"size\": null, \"fdr_col\": null}, "
        "\"code\": \"df.columns = [c.strip().lower() for c in df.columns]\\nx_col = find_column_by_keywords(df, ['temp', 'celsius', 'temperature']) or normalize_column('temperature', tuple(df.columns)) or normalize_column('temp', tuple(df.columns)) or normalize_column('temp (°c)', tuple(df.columns)) or (pick_default_numeric(df, 2)[0] if pick_default_numeric(df, 2) else None)\\ny_col = find_column_by_keywords(df, ['mass', 'weight']) or normalize_column('mass', tuple(df.columns)) or normalize_column('mass (%)', tuple(df.columns)) or (pick_default_numeric(df, 2)[1] if len(pick_default_numeric(df, 2)) > 1 else None)\\nif x_col is None or y_col is None:\\n    raise HTTPException(status_code=400, detail='Insufficient numeric columns for TGA plot. Ensure dataset contains columns with keywords like temp, celsius, or mass.')\\nif not pd.api.types.is_numeric_dtype(df[x_col]) or not pd.api.types.is_numeric_dtype(df[y_col]):\\n    raise HTTPException(status_code=400, detail=f'Columns {x_col} and {y_col} must be numeric.')\\nif df[x_col].isna().any() or df[y_col].isna().any():\\n    raise HTTPException(status_code=400, detail=f'Columns {x_col} or {y_col} contain NaN values. Please remove or impute missing values.')\\nif len(df[x_col].unique()) < 2:\\n    raise HTTPException(status_code=400, detail=f'Column {x_col} has insufficient unique values. Ensure at least 2 unique values.')\\ndf_sorted = df.sort_values(x_col)\\nfig, ax = plt.subplots(figsize=(6, 4))\\nax.plot(df_sorted[x_col], df_sorted[y_col], marker='', linestyle='-')\\nax.set_title('TGA Curve')\\nax.set_xlabel('Temperature (°C)')\\nax.set_ylabel('Mass (%)')\\nax.grid(True)\\nfig.tight_layout()\\nfig.savefig(buf, format=fmt.value, dpi=300)\\nplt.close(fig)\"}"
        "Example output for dtg: "
        "{\"spec\": {\"chart_type\": \"dtg\", \"x\": \"temp (°c)\", \"y\": \"dmass/dt (%)\", \"group\": null, \"bins\": null, \"title\": \"DTG Curve\", \"x_label\": \"Temperature (°C)\", \"y_label\": \"Derivative Mass (%/°C)\", \"color\": null, \"size\": null, \"fdr_col\": null}, "
        "\"code\": \"df.columns = [c.strip().lower() for c in df.columns]\\nx_col = find_column_by_keywords(df, ['temp', 'celsius', 'temperature']) or normalize_column('temperature', tuple(df.columns)) or normalize_column('temp', tuple(df.columns)) or normalize_column('temp (°c)', tuple(df.columns)) or (pick_default_numeric(df, 2)[0] if pick_default_numeric(df, 2) else None)\\ny_col = find_column_by_keywords(df, ['dmass', 'derivative']) or normalize_column('dmass/dt (%)', tuple(df.columns)) or normalize_column('mass', tuple(df.columns)) or normalize_column('mass (%)', tuple(df.columns)) or (pick_default_numeric(df, 2)[1] if len(pick_default_numeric(df, 2)) > 1 else None)\\nif x_col is None or y_col is None:\\n    raise HTTPException(status_code=400, detail='Insufficient numeric columns for DTG plot. Ensure dataset contains columns with keywords like temp, celsius, mass, or derivative.')\\nif not pd.api.types.is_numeric_dtype(df[x_col]) or not pd.api.types.is_numeric_dtype(df[y_col]):\\n    raise HTTPException(status_code=400, detail=f'Columns {x_col} and {y_col} must be numeric.')\\nif df[x_col].isna().any() or df[y_col].isna().any():\\n    raise HTTPException(status_code=400, detail=f'Columns {x_col} or {y_col} contain NaN values. Please remove or impute missing values.')\\nif len(df[x_col].unique()) < 2:\\n    raise HTTPException(status_code=400, detail=f'Column {x_col} has insufficient unique values. Ensure at least 2 unique values.')\\ndf_sorted = df.sort_values(x_col)\\ndmass_col = find_column_by_keywords(df, ['dmass', 'derivative']) or normalize_column('dmass/dt (%)', tuple(df.columns))\\nif dmass_col:\\n    dmass_dt = df_sorted[dmass_col]\\nelse:\\n    dmass_dt = gradient(df_sorted[y_col], df_sorted[x_col])\\nfig, ax = plt.subplots(figsize=(6, 4))\\nax.plot(df_sorted[x_col], dmass_dt, marker='', linestyle='-')\\nax.set_title('DTG Curve')\\nax.set_xlabel('Temperature (°C)')\\nax.set_ylabel('Derivative Mass (%/°C)')\\nax.grid(True)\\nfig.tight_layout()\\nfig.savefig(buf, format=fmt.value, dpi=300)\\nplt.close(fig)\"}"
        "Example output for tga_dtg_combined: "
        "{\"spec\": {\"chart_type\": \"tga_dtg_combined\", \"x\": \"temp (°c)\", \"y\": \"mass (%)\", \"group\": null, \"bins\": null, \"title\": \"TGA and DTG Curves\", \"x_label\": \"Temperature (°C)\", \"y_label\": \"Mass (%)\", \"color\": null, \"size\": null, \"fdr_col\": null}, "
        "\"code\": \"df.columns = [c.strip().lower() for c in df.columns]\\nx_col = find_column_by_keywords(df, ['temp', 'celsius', 'temperature']) or normalize_column('temperature', tuple(df.columns)) or normalize_column('temp', tuple(df.columns)) or normalize_column('temp (°c)', tuple(df.columns)) or (pick_default_numeric(df, 2)[0] if pick_default_numeric(df, 2) else None)\\ny_col = find_column_by_keywords(df, ['mass', 'weight']) or normalize_column('mass', tuple(df.columns)) or normalize_column('mass (%)', tuple(df.columns)) or (pick_default_numeric(df, 2)[1] if len(pick_default_numeric(df, 2)) > 1 else None)\\ndmass_col = find_column_by_keywords(df, ['dmass', 'derivative']) or normalize_column('dmass/dt (%)', tuple(df.columns))\\nif x_col is None or y_col is None:\\n    raise HTTPException(status_code=400, detail='Insufficient numeric columns for TGA-DTG combined plot. Ensure dataset contains columns with keywords like temp, celsius, mass, or derivative.')\\nif not pd.api.types.is_numeric_dtype(df[x_col]) or not pd.api.types.is_numeric_dtype(df[y_col]):\\n    raise HTTPException(status_code=400, detail=f'Columns {x_col} and {y_col} must be numeric.')\\nif df[x_col].isna().any() or df[y_col].isna().any():\\n    raise HTTPException(status_code=400, detail=f'Columns {x_col} or {y_col} contain NaN values. Please remove or impute missing values.')\\nif len(df[x_col].unique()) < 2:\\n    raise HTTPException(status_code=400, detail=f'Column {x_col} has insufficient unique values. Ensure at least 2 unique values.')\\nif dmass_col and not pd.api.types.is_numeric_dtype(df[dmass_col]):\\n    raise HTTPException(status_code=400, detail=f'Derivative column {dmass_col} must be numeric.')\\nif dmass_col and df[dmass_col].isna().any():\\n    raise HTTPException(status_code=400, detail=f'Derivative column {dmass_col} contains NaN values. Please remove or impute missing values.')\\ndf_sorted = df.sort_values(x_col)\\nif dmass_col:\\n    dmass_dt = df_sorted[dmass_col]\\nelse:\\n    dmass_dt = gradient(df_sorted[y_col], df_sorted[x_col])\\nfig, ax1 = plt.subplots(figsize=(6, 4))\\nax1.plot(df_sorted[x_col], df_sorted[y_col], marker='', linestyle='-', color='blue', label='TGA')\\nax1.set_xlabel('Temperature (°C)')\\nax1.set_ylabel('Mass (%)', color='blue')\\nax1.tick_params(axis='y', labelcolor='blue')\\nax2 = ax1.twinx()\\nax2.plot(df_sorted[x_col], dmass_dt, marker='', linestyle='-', color='red', label='DTG')\\nax2.set_ylabel('Derivative Mass (%/°C)', color='red')\\nax2.tick_params(axis='y', labelcolor='red')\\nlines1, labels1 = ax1.get_legend_handles_labels()\\nlines2, labels2 = ax2.get_legend_handles_labels()\\nax1.legend(lines1 + lines2, labels1 + labels2, loc='best')\\nax1.set_title('TGA and DTG Curves')\\nax1.grid(True)\\nfig.tight_layout()\\nfig.savefig(buf, format=fmt.value, dpi=300)\\nplt.close(fig)\"}"
    )
    user_prompt = (
        f"Instruction: {instruction}\n"
        "Return a valid JSON with 'spec' (matching the ChartSpec model) and 'code' (full Python code string for rendering, without imports)."
    )
    content = ""  # Initialize content to avoid UnboundLocalError
    try:
        resp = client.chat.completions.create(
            model="deepseek/deepseek-r1:free",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0,
        )
        content = resp.choices[0].message.content
        print(f"Raw API response: {content}")
        content = content.strip()
        if content.startswith("```json"):
            content = content[7:-3].strip()
        elif content.startswith("```"):
            content = content[3:-3].strip()
        match = re.match(r'\{(?:[^{}]|\{[^{}]*\})*\}', content)
        if not match:
            raise ValueError("No valid JSON object found in response")
        content = match.group(0)
        if not content:
            raise ValueError("Empty response from DeepSeek R1 after cleaning")
        result = json.loads(content)
        if 'spec' not in result or 'code' not in result:
            raise ValueError("Invalid response: missing 'spec' or 'code'")
        return result
    except json.JSONDecodeError as e:
        print(f"JSONDecodeError: {e}, Raw content: {content}")
        raise HTTPException(500, detail=f"Failed to parse DeepSeek R1 response as JSON: {e}")
    except Exception as e:
        print(f"API Error: {e}, Raw content: {content}")
        raise HTTPException(500, detail=f"DeepSeek R1 API failed: {e}")

# ---------------------------
# LLM Router for Dataset Questions
# ---------------------------
def llm_dataset_qa(question: str, df: pd.DataFrame) -> str:
    question_lower = question.lower().strip()

    # Helper function to format table
    def format_table(headers: list, rows: list) -> str:
        table = "| " + " | ".join(headers) + " |\n"
        table += "| " + " | ".join(["---"] * len(headers)) + " |\n"
        for row in rows:
            table += "| " + " | ".join([f"{x:.2f}" if isinstance(x, (int, float)) else str(x) for x in row]) + " |\n"
        return table

    # Handle specific column statistic requests (e.g., "show mean of age", "show median of age", "show mode of age")
    stat_match = re.match(r"show\s+(mean|median|mode)\s+of\s+([a-zA-Z0-9_]+)", question_lower)
    if stat_match:
        stat_type, col = stat_match.groups()
        normalized_col = normalize_column(col, tuple(df.columns))
        if normalized_col and normalized_col in df.columns:
            if pd.api.types.is_numeric_dtype(df[normalized_col]):
                if stat_type == "mean":
                    value = df[normalized_col].mean()
                elif stat_type == "median":
                    value = df[normalized_col].median()
                elif stat_type == "mode":
                    mode_series = df[normalized_col].mode()
                    value = mode_series[0] if not mode_series.empty else "No mode"
                return format_table(
                    headers=["Column", stat_type.capitalize()],
                    rows=[[normalized_col, value]]
                )
            else:
                return f"Column '{normalized_col}' is not numeric."
        else:
            return f"Column '{col}' not found in dataset."

    # Handle request for statistics in table format (e.g., "show mean in table", "show median in table", "show mode in table")
    stat_table_match = re.match(r"show\s+(mean|median|mode)\s+in\s+table", question_lower)
    if stat_table_match:
        stat_type = stat_table_match.group(1)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if not numeric_cols.empty:
            if stat_type == "mean":
                values = df[numeric_cols].mean()
            elif stat_type == "median":
                values = df[numeric_cols].median()
            elif stat_type == "mode":
                values = df[numeric_cols].mode().iloc[0] if not df[numeric_cols].mode().empty else pd.Series(index=numeric_cols)
            rows = [[col, values.get(col, "No mode" if stat_type == "mode" else np.nan)] for col in numeric_cols]
            return format_table(
                headers=["Column", stat_type.capitalize()],
                rows=rows
            )
        else:
            return "No numeric columns found in dataset."

    # Handle request for executive summary as table
    if "generate executive summary as table" in question_lower:
        rows = []
        # Dataset Shape
        rows.append(["Rows", str(df.shape[0])])
        rows.append(["Columns", str(df.shape[1])])
        # Numeric Columns Count
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        rows.append(["Numeric Columns", str(len(numeric_cols))])
        # Non-Numeric Columns Count
        non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns
        rows.append(["Non-Numeric Columns", str(len(non_numeric_cols))])
        # Missing Values
        total_missing = df.isnull().sum().sum()
        rows.append(["Total Missing Values", str(total_missing)])
        # Mean of Numeric Columns (if any)
        if not numeric_cols.empty:
            mean_value = df[numeric_cols].mean().mean()
            rows.append(["Average Mean (Numeric)", f"{mean_value:.2f}" if pd.notna(mean_value) else "N/A"])
            median_value = df[numeric_cols].median().median()
            rows.append(["Average Median (Numeric)", f"{median_value:.2f}" if pd.notna(median_value) else "N/A"])
            mode_series = df[numeric_cols].mode()
            mode_value = mode_series.iloc[0].mean() if not mode_series.empty else np.nan
            rows.append(["Average Mode (Numeric)", f"{mode_value:.2f}" if pd.notna(mode_value) else "No mode"])
        else:
            rows.extend([
                ["Average Mean (Numeric)", "N/A"],
                ["Average Median (Numeric)", "N/A"],
                ["Average Mode (Numeric)", "N/A"]
            ])
        return format_table(
            headers=["Metric", "Value"],
            rows=rows
        )

    # Handle EDA analysis request (e.g., "generate eda analysis")
    if "generate eda analysis" in question_lower:
        eda = []

        # Dataset Shape
        eda.append("**Dataset Shape**:")
        rows = [["Rows", df.shape[0]], ["Columns", df.shape[1]]]
        eda.append(format_table(["Property", "Value"], rows))
        
        # Column Data Types
        eda.append("\n**Column Data Types**:")
        rows = [[col, str(dtype)] for col, dtype in df.dtypes.items()]
        eda.append(format_table(["Column", "Data Type"], rows))
        
        # Summary Statistics for Numeric Columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if not numeric_cols.empty:
            eda.append("\n**Summary Statistics for Numeric Columns**:")
            stats = df[numeric_cols].describe().T
            headers = ["Column", "Count", "Mean", "Std", "Min", "25%", "50%", "75%", "Max"]
            rows = [
                [col, row['count'], row['mean'], row['std'], row['min'], row['25%'], row['50%'], row['75%'], row['max']]
                for col, row in stats.iterrows()
            ]
            eda.append(format_table(headers, rows))
        
            # Median Values
            eda.append("\n**Median Values**:")
            medians = df[numeric_cols].median()
            rows = [[col, medians[col]] for col in numeric_cols]
            eda.append(format_table(["Column", "Median"], rows))
            
            # Mode Values
            eda.append("\n**Mode Values**:")
            modes = df[numeric_cols].mode().iloc[0] if not df[numeric_cols].mode().empty else pd.Series(index=numeric_cols)
            rows = [[col, modes.get(col, "No mode")] for col in numeric_cols]
            eda.append(format_table(["Column", "Mode"], rows))
        
        # Missing Values
        eda.append("\n**Missing Values**:")
        missing = df.isnull().sum()
        rows = [[col, count] for col, count in missing.items()]
        if missing.sum() == 0:
            rows = [["All Columns", "No missing values"]]
        eda.append(format_table(["Column", "Missing Count"], rows))
        
        return "\n".join(eda)

    # Existing QA logic for other questions
    system_prompt = (
        "You are an expert assistant that answers questions about a dataset based on its structure and content. "
        "The dataset has columns: " + ", ".join(df.columns.astype(str)) + ". "
        "Provide a concise, accurate answer to the user's question. For questions requiring computation (e.g., summary statistics, counts, or correlations), perform the analysis using pandas and return the result as a markdown table if appropriate, or as a string otherwise. "
        "For statistical questions (e.g., mean, median, mode, std, min, max), return results in a markdown table with columns [Column, Statistic]. "
        "For questions about column names, data types, or row counts, provide direct answers as strings. "
        "Do not generate plots or code; return only the answer as a string. "
        "Example table for stats: | Column | Statistic |\n|--------|-----------|\n| col1   | 10.5      |\n"
        "Example non-table answers: "
        "- Q: 'What are the columns?' A: 'Columns: col1, col2, col3' "
        "- Q: 'How many rows?' A: 'Number of rows: 100' "
    )
    user_prompt = (
        f"Question: {question}\n"
        f"Dataset info: {df.shape[0]} rows, columns: {', '.join(df.columns.astype(str))} with dtypes: {df.dtypes.to_dict()}\n"
        "Answer the question concisely as a string, using a markdown table for statistical computations where appropriate."
    )
    content = ""  # Initialize content to avoid UnboundLocalError
    try:
        resp = client.chat.completions.create(
            model="deepseek/deepseek-r1:free",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0,
        )
        content = resp.choices[0].message.content
        return content.strip()
    except Exception as e:
        print(f"QA API Error: {e}, Raw content: {content}")
        raise HTTPException(500, detail=f"Failed to process dataset question: {e}")

# ---------------------------
# Chart Rendering
# ---------------------------
def render_chart(df: pd.DataFrame, result: Dict[str, Any], fmt: OutputFormat) -> bytes:
    spec_dict = result['spec']
    code = result['code']
    try:
        spec = ChartSpec(**spec_dict)
    except Exception as e:
        raise HTTPException(400, detail=f"Invalid chart specification: {str(e)}")
    buf = io.BytesIO()
    from scipy.optimize import curve_fit
    from scipy.interpolate import UnivariateSpline
    safe_globals = {
        '__builtins__': {
            'len': len,
            'min': min,
            'max': max,
            'all': all,
            'enumerate': enumerate,
            'set': set,
            'tuple': tuple,
        },
        'io': io,
        'np': np,
        'pd': pd,
        'plt': matplotlib.pyplot,
        'sns': sns,
        're': re,
        'curve_fit': curve_fit,
        'UnivariateSpline': UnivariateSpline,
        'gradient': np.gradient,
        'umap': umap,
        'roc_curve': roc_curve,
        'auc': auc,
        'PCA': PCA,
        'df': df,
        'spec': spec,
        'buf': buf,
        'fmt': fmt,
        'normalize_column': normalize_column,
        'pick_default_numeric': pick_default_numeric,
        'find_column_by_keywords': find_column_by_keywords,
        'ChartType': ChartType,
        'HTTPException': HTTPException,
    }
    try:
        exec(code, safe_globals)
        buf.seek(0)
        return buf.read()
    except Exception as e:
        print(f"Execution error: {e}")
        raise HTTPException(500, detail=f"Failed to execute AI-generated chart code: {str(e)}")
    finally:
        plt.close('all')

# ---------------------------
# PDF Report Generation
# ---------------------------
def export_customer_report_pdf(result, customer_name, logo_path=None, output_path="customer_report.pdf"):
    doc = SimpleDocTemplate(output_path, pagesize=A4)
    styles = getSampleStyleSheet()
    story = []

    # Cover Page
    story.append(Paragraph(f"{customer_name} Report", styles['Title']))
    story.append(Paragraph(f"Prepared by Innov® Labs<br/>{datetime.today().strftime('%Y-%m-%d')}", styles['Normal']))
    if logo_path:
        story.append(Image(logo_path, width=100, height=100))
    story.append(Spacer(1, 40))

    # Executive Summary
    story.append(Paragraph("Executive Summary", styles['Heading1']))
    story.append(Paragraph(result.get("summary", "N/A"), styles['Normal']))
    story.append(Spacer(1, 20))

    # Data Insights
    story.append(Paragraph("Data Insights", styles['Heading1']))
    for insight in result.get("insights", []):
        story.append(Paragraph(f"• {insight}", styles['Normal']))
    story.append(Spacer(1, 20))

    # Charts
    story.append(Paragraph("Charts", styles['Heading1']))
    for chart_path in result.get("charts", []):
        story.append(Image(chart_path, width=400, height=250))
        story.append(Spacer(1, 12))

    # Conclusion
    story.append(Paragraph("Conclusion", styles['Heading1']))
    story.append(Paragraph(result.get("conclusion", "N/A"), styles['Normal']))
    story.append(Spacer(1, 20))

    # Verification
    story.append(Paragraph("Verification", styles['Heading1']))
    story.append(Paragraph("Prepared by: Innov® Labs", styles['Normal']))
    story.append(Paragraph("Verified by: __________________ (Signature)", styles['Normal']))
    story.append(Paragraph(f"Timestamp: {datetime.now().isoformat()}", styles['Normal']))

    doc.build(story)

# ---------------------------
# Endpoints
# ---------------------------
@app.post("/upload")
async def upload(file: UploadFile = File(...)) -> Dict[str, Any]:
    try:
        content = await file.read()
        if file.filename.lower().endswith(".csv"):
            df = pd.read_csv(io.BytesIO(content))
        elif file.filename.lower().endswith((".xlsx", ".xls")):
            df = pd.read_excel(io.BytesIO(content))
        else:
            raise HTTPException(400, detail="Only CSV/XLSX supported.")
        df.columns = [c.strip().lower() for c in df.columns]
        dataset_id = str(uuid.uuid4())
        DATASETS[dataset_id] = df
        return {"dataset_id": dataset_id, "columns": df.columns.tolist()}
    except Exception as e:
        raise HTTPException(400, detail=f"Failed to read file: {e}")

@app.post("/plot")
async def plot(req: RenderRequest):
    df = DATASETS.get(req.dataset_id)
    if df is None:
        raise HTTPException(404, detail="dataset_id not found")
    result = llm_router(req.instruction, df)
    img = render_chart(df, result, req.format)
    
    # Save chart to disk
    output_dir = "charts"
    os.makedirs(output_dir, exist_ok=True)
    chart_filename = f"chart_{req.dataset_id}_{uuid.uuid4()}.{req.format}"
    chart_path = os.path.join(output_dir, chart_filename)
    with open(chart_path, "wb") as f:
        f.write(img)
    
    # Store chart metadata
    if req.dataset_id not in CHART_HISTORY:
        CHART_HISTORY[req.dataset_id] = []
    CHART_HISTORY[req.dataset_id].append({
        "chart_type": result["spec"]["chart_type"],
        "image_path": chart_path,
        "instruction": req.instruction
    })
    
    media_type = "image/png" if req.format == OutputFormat.png else "image/svg+xml"
    return StreamingResponse(io.BytesIO(img), media_type=media_type)

@app.post("/ask")
def ask(req: AskRequest):
    df = DATASETS.get(req.dataset_id)
    if df is None:
        raise HTTPException(404, detail="dataset_id not found")
    answer = llm_dataset_qa(req.question, df)
    return {"answer": answer}

@app.post("/generate_report")
async def generate_report(req: ReportRequest):
    df = DATASETS.get(req.dataset_id)
    if df is None:
        raise HTTPException(404, detail="dataset_id not found")
    
    charts = CHART_HISTORY.get(req.dataset_id, [])
    if not charts:
        raise HTTPException(400, detail="No charts generated in this session")
    
    # Generate summary if default
    if req.summary == "Generated by Innov® Labs":
        summary_question = "Provide a short executive summary of the dataset."
        req.summary = llm_dataset_qa(summary_question, df)
    
    # Generate insights
    insights_question = "Provide key data insights as a list of bullet points."
    insights_answer = llm_dataset_qa(insights_question, df)
    insights = []
    for line in insights_answer.split('\n'):
        stripped = line.strip()
        if stripped.startswith(('-', '•')):
            insights.append(stripped[1:].strip())
    
    # Add session info to insights
    session_insights = [
        f"Generated {len(charts)} charts in this session.",
        f"Columns analyzed: {', '.join(df.columns)}",
        f"Dataset size: {df.shape[0]} rows"
    ]
    insights += session_insights
    
    # Generate conclusion if default
    if req.conclusion == "Analysis completed successfully.":
        conclusion_question = "Provide a concluding statement for the dataset analysis."
        req.conclusion = llm_dataset_qa(conclusion_question, df)
    
    # Prepare result dictionary
    result = {
        "summary": req.summary,
        "insights": insights,
        "charts": [chart["image_path"] for chart in charts],
        "chart_details": [(chart["chart_type"], chart["instruction"]) for chart in charts],
        "conclusion": req.conclusion
    }
    
    output_dir = "reports"
    os.makedirs(output_dir, exist_ok=True)
    sanitized_filename = re.sub(r'[^a-zA-Z0-9-_]', '_', req.filename)
    output_filename = f"{sanitized_filename}.{req.format}"
    output_path = os.path.join(output_dir, output_filename)
    
    if req.format == "docx":
        doc = Document()
        # Cover Page
        doc.add_heading(f"{req.customer_name} Report", level=0)
        doc.add_paragraph(f"Prepared for: {req.customer_name}\nPrepared by Innov® Labs\nDate: {datetime.today().strftime('%Y-%m-%d')}")
        if req.logo_path and os.path.exists(req.logo_path):
            doc.add_picture(req.logo_path, width=Inches(1.5))
        doc.add_page_break()
        
        # Executive Summary
        doc.add_heading("Executive Summary", level=1)
        doc.add_paragraph(result["summary"])
        
        # Data Insights
        doc.add_heading("Data Insights", level=1)
        for insight in result["insights"]:
            doc.add_paragraph(f"• {insight}")
        
        # Charts
        doc.add_heading("Charts", level=1)
        doc.add_paragraph(f"Number of Charts Generated: {len(charts)}")
        for i, (chart_path, (chart_type, instruction)) in enumerate(zip(result["charts"], result["chart_details"]), 1):
            if os.path.exists(chart_path):
                doc.add_picture(chart_path, width=Inches(5))
                doc.add_paragraph(f"Figure {i}: {chart_type.capitalize()} chart (Instruction: {instruction})")
        
        # Conclusion
        doc.add_heading("Conclusion", level=1)
        doc.add_paragraph(result["conclusion"])
        
        # Verification
        doc.add_heading("Verification", level=1)
        doc.add_paragraph("Prepared by: Innov® Labs")
        doc.add_paragraph("Verified by: __________________ (Signature)")
        doc.add_paragraph(f"Timestamp: {datetime.now().isoformat()}")
        
        doc.save(output_path)
        media_type = "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    
    else:  # PDF
        export_customer_report_pdf(result, req.customer_name, req.logo_path, output_path=output_path)
        media_type = "application/pdf"
    
    return StreamingResponse(
        open(output_path, "rb"),
        media_type=media_type,
        headers={"Content-Disposition": f"attachment; filename={output_filename}"}
    )
